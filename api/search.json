[{"id":"64e1a25088b85c68e2e16be94c5eff1a","title":"kubernetes的架构设计","content":"Master节点：Master节点是K8s集群的控制节点，在生产环境中不建议部署集群核心组件以外的任何容器，公司业务的容器更是不要部署到master节点上，以免升级或者维护时对业务造成影响：\nkube-apiserver：整个集群的控制中枢\n提供集群中各个模块之间的数据交换，并将集群状态和信息存储到分布式键值对（key-value）存储到etcd集群中。\n集群管理，资源配额，集群安全机制的入口，为集群各类资源对象提供增删改查以及watch的REST API接口\n\n\nkube-controller-manager ：集群状态管理器\n保证Pod或其他资源达到期望值，当Pod的副本数或者其他资源因故障和错误导致无法正常运行，没有达到设定的值时，Controller-manager会尝试自动修复达到期望值\nController-manager包含 Node controller ，ReplicationController，EndpointController，NamespaceController，ServiceAccountController，RescueQuotaController，ServiceController和TokenController等\n\n\nkube-scheduler：集群pod的调度中心\n通过调度算法将pod分配最佳node节点上，它通过APIServer监听所有pod的状态\n如果有pod未被调度到任何一个node节点上面，那么Scheduler会通过策略选择最佳node节点进行调度，对每一个Pod创建一个绑定 然后被调度的节点上的kubelet负责启动该pod\nScheduler是集群可插拔组件，它跟踪每个节点上的资源利用率以确保工作负载不会超过可用资源。\nScheduler在调度过程中需要考虑公平，资源高效利用，效率等方面的问题\n\n\netcd：数据存储组件\n持久型，轻量型，分布式的键值（key-value）数据存储组件\netcd高可用是k8s集群中至关重要的一环，在生产环境中建议部署大于3的奇数的etcd集群，来保障数据的安全性和可恢复性。\n大规模集群环境建议部署etcd集群在k8s集群外，使用高性能服务器部署提高etcd性能和延迟\n\n\n\nnoder节点：Node节点主要负责部署运行容器，确保每个节点上都有可运行的docker容器。Node节点包括以下组件：\nkubelet：\n负责与Master节点通信协作，管理该节点上的Pod，对容器进行健康检查及监控，同时负责上报Pod的状态\n\n\nkube-proxy：\n负责Pod之间的通信和负载均衡，将指定流量发到后的正确的机器上\n\n\nDocker Engine：\nDocker引擎，负载对容器的管理\n\n\n\nCalico：符合CNI标准的网络插件，负责给每个pod分配不重复的IPAddons：Addons是Kubernetes提供的可选组件，用于扩展集群功能，包括：\nDNS：提供集群内部DNS服务；\nDashboard：提供集群管理和监控功能；\nIngress Controller：提供HTTP和HTTPS流量的路由和负载均衡服务；\nMetrics Server：收集集群中的度量指标。\n\nKubernetes架构的设计旨在实现高可用性和可扩展性。Master节点使用高可用性的配置，包括多个实例和负载均衡器，以确保在Master节点出现故障时，集群仍然可以正常运行。Node节点可以根据需要添加或删除，以实现集群的水平扩展。\n总之，Kubernetes架构是一个分层的设计，旨在为容器化应用程序提供高可用性、可靠性和可扩展性。各个组件和角色之间相互协作，共同构建一个功能强大的容器编排平台。\n","slug":"kubernetes的架构设计","date":"2023-04-14T14:32:15.000Z","categories_index":"","tags_index":"Kubernetes","author_index":"xiaomaomi"},{"id":"d670df397553a310eb51aea95c3eefc2","title":"高可用部署k8s集群（二进制）","content":"（一）环境准备集群节点介绍\n\n\nIP地址\n机器角色\n集群组件\n\n\n\n\n192.168.2.188\nVIP\nHAProxy keepalived\ncentos7.5\n\n\n192.168.2.18\nMaster1\nkube-apiserver kube-controller-manage kube-scheduler etcd haproxy keepalived docker\ncentos7.5\n\n\n192.168.2.19\nmaster2\nkube-apiserver kube-controller-manage kube-scheduler etcd haproxy keepalived dcoker\ncentos7.5\n\n\n192.168.2.20\nmaster3\nkube-apiserver kube-controller-manage kube-scheduler etcd haproxy keepalived dcoker\ncentos7.5\n\n\n192.168.2.21\nnode1\nkube-proxy、kubelet\ncentos7.5\n\n\n192.168.2.22\nnode2\nkube-proxy、kubelet\ncentos7.5\n\n\n192.168.2.23\nnode3\nkube-proxy、kubelet\ncentos7.5\n\n\n软件版本\n\n\n软件\n版本\n\n\n\ncentos 7.5\n内核：5.19.x\n\n\nkube-apiserver、kube-controller-manager、kube-scheduler、kubelet、kube-proxy\n1.22.1\n\n\netcd\n3.5.0\n\n\ncalico\n3.19.1\n\n\ncoredns\n1.8.4\n\n\ndocker\n20.10.9\n\n\nhaproxy\n1.5.18\n\n\nkeepalived\n1.3.5\n\n\n网络分配**\t**\n\n\n网段信息\n配置\n\n\n\nPod网段\n172.168.0.0&#x2F;12\n\n\nService网段\n10.96.0.0&#x2F;16\n\n\n（二）基础准备配置免密#!&#x2F;bin&#x2F;bash\n#机器IP\nIP&#x3D;&#96;cat &#x2F;root&#x2F;shell&#x2F;ip.txt&#96;\n#用户名，密码\nusername&#x3D;root\npasswd&#x3D;1234\n\nPS3&#x3D;&quot;请选择要执行的操作：&quot;\noptions&#x3D;(&quot;生成密钥&quot; &quot;分发密钥&quot; &quot;远程命令执行&quot; &quot;退出&quot;)\n\nwhile true; do\n    select opt in &quot;$&#123;options[@]&#125;&quot;\n    do\n        case $opt in\n            &quot;生成密钥&quot;)\n                echo &quot;正在执行生成密钥...&quot;\n                #生成密钥\n                expect &lt;&lt; EOF\n                    spawn ssh-keygen\n                    expect &#123;\n                            &quot;*(&#x2F;root&#x2F;.ssh&#x2F;id_rsa)&quot; &#123;send &quot;\\r&quot;; exp_continue&#125;\n                            &quot;*(empty for no passphrase)&quot; &#123;send &quot;\\r&quot;; exp_continue&#125;\n                            &quot;*again&quot; &#123;send &quot;\\r&quot;&#125;\n                    &#125;\n                    expect eof\nEOF\n                ;;\n            &quot;分发密钥&quot;)\n                echo &quot;正在执行分发密钥...&quot;\n                #分发密钥\n                for i in $&#123;IP&#125;; do\n                    expect &lt;&lt; EOF\n                        spawn ssh-copy-id -i &#x2F;root&#x2F;.ssh&#x2F;id_rsa.pub $&#123;username&#125;@$&#123;i&#125;\n                        expect &#123;\n                            &quot;(yes&#x2F;no)? &quot; &#123;\n                                send &quot;yes\\r&quot;\n                                expect &quot;*assword:&quot; &#123;\n                                    send &quot;$&#123;passwd&#125;\\r&quot;\n                                &#125;\n                            &#125;\n                            &quot;*assword:&quot; &#123;\n                                send &quot;$&#123;passwd&#125;\\r&quot;\n                            &#125;\n                        &#125;\n  \t            expect eof\nEOF\n                done\n                ;;\n           &quot;远程命令执行&quot;)\n                      echo &quot;执行远程命令&quot;\n                      echo &quot;输入你要运行的指令&quot;\n                      read ter\n                      for i in $IP; do\n                          echo &quot;当前服务器IP：&quot; $i\n                          ssh $username@$i &quot;$ter&quot;\n                      done\n                      ;;\n           &quot;退出&quot;)\n                echo &quot;退出程序&quot;\n                break 2\n                ;;\n            *)\n                echo &quot;无效的选项 $REPLY&quot;\n                ;;\n        esac\n    done\ndone\n\n**#### **\n修改主机名hostnamectl set-hostname xxx #按照机器规划修改\n\n配置hostscat &gt;&gt; &#x2F;etc&#x2F;hosts &lt;&lt; EOF\n192.168.2.18 master1\n192.168.2.19 master2\n192.168.2.20 msater3\n192.168.2.21 node1\n192.168.2.22 node2\n192.168.2.23 node3\nEOF\n\n关闭firewalld，dnsmasq，selinux，NetworkManagersystemctl disable --now firewalld \nsystemctl disable --now dnsmasq\nsystemctl disable --now NetworkManager\n\nsetenforce 0\nsed -i &#39;s#SELINUX&#x3D;enforcing#SELINUX&#x3D;disabled#g&#39; &#x2F;etc&#x2F;sysconfig&#x2F;selinux\nsed -i &#39;s#SELINUX&#x3D;enforcing#SELINUX&#x3D;disabled#g&#39; &#x2F;etc&#x2F;selinux&#x2F;config\n\n关闭swapswapoff -a &amp;&amp; sysctl -w vm.swappiness&#x3D;0\nsed -ri &#39;s&#x2F;.*swap.*&#x2F;#&amp;&#x2F;&#39; &#x2F;etc&#x2F;fstab  \n\n安装必要软件rpm -ivh http:&#x2F;&#x2F;mirrors.wlnmp.com&#x2F;centos&#x2F;wlnmp-release-centos.noarch.rpm\nyum install expect ntpdate wget jq psmisc vim ntpdate net-tools telnet yum-utils device-mapper-persistent-data lvm2 git lrzsz ipvsadm ipset sysstat conntrack libseccomp -y\n\n时间同步ln -sf &#x2F;usr&#x2F;share&#x2F;zoneinfo&#x2F;Asia&#x2F;Shanghai &#x2F;etc&#x2F;localtime\necho &#39;Asia&#x2F;Shanghai&#39; &gt;&#x2F;etc&#x2F;timezone\nntpdate time2.aliyun.com\n#添加定时任务 同步时间\ncrontab -e\n*&#x2F;5 * * * * &#x2F;usr&#x2F;sbin&#x2F;ntpdate time2.aliyun.com\n\n系统配置#limit优化\nulimit -SHn 65535\n\ncat &lt;&lt;EOF &gt;&gt; &#x2F;etc&#x2F;security&#x2F;limits.conf\n* soft nofile 655360\n* hard nofile 131072\n* soft nproc 655350\n* hard nproc 655350\n* soft memlock unlimited\n* hard memlock unlimited\nEOF\n\n加载ipvsmodprobe -- ip_vs \nmodprobe -- ip_vs_rr \nmodprobe -- ip_vs_wrr \nmodprobe -- ip_vs_sh \nmodprobe -- nf_conntrack \n\n\n#创建 &#x2F;etc&#x2F;modules-load.d&#x2F;ipvs.conf 并加入以下内容： \ncat &gt;&#x2F;etc&#x2F;modules-load.d&#x2F;ipvs.conf &lt;&lt;EOF \nip_vs \nip_vs_lc \nip_vs_wlc \nip_vs_rr \nip_vs_wrr \nip_vs_lblc \nip_vs_lblcr \nip_vs_dh \nip_vs_sh \nip_vs_fo \nip_vs_nq \nip_vs_sed \nip_vs_ftp \nip_vs_sh \nnf_conntrack \nip_tables \nip_set \nxt_set \nipt_set \nipt_rpfilter \nipt_REJECT \nipip \nEOF\n\n#设置为开机启动\nsystemctl enable --now systemd-modules-load.service\n\nk8s内核优化cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf\nnet.ipv4.ip_forward &#x3D; 1\nnet.bridge.bridge-nf-call-iptables &#x3D; 1\nnet.bridge.bridge-nf-call-ip6tables &#x3D; 1\nfs.may_detach_mounts &#x3D; 1\nvm.overcommit_memory&#x3D;1\nvm.panic_on_oom&#x3D;0\nfs.inotify.max_user_watches&#x3D;89100\nfs.file-max&#x3D;52706963\nfs.nr_open&#x3D;52706963\nnet.netfilter.nf_conntrack_max&#x3D;2310720\n\nnet.ipv4.tcp_keepalive_time &#x3D; 600\nnet.ipv4.tcp_keepalive_probes &#x3D; 3\nnet.ipv4.tcp_keepalive_intvl &#x3D;15\nnet.ipv4.tcp_max_tw_buckets &#x3D; 36000\nnet.ipv4.tcp_tw_reuse &#x3D; 1\nnet.ipv4.tcp_max_orphans &#x3D; 327680\nnet.ipv4.tcp_orphan_retries &#x3D; 3\nnet.ipv4.tcp_syncookies &#x3D; 1\nnet.ipv4.tcp_max_syn_backlog &#x3D; 16384\nnet.ipv4.ip_conntrack_max &#x3D; 131072\nnet.ipv4.tcp_max_syn_backlog &#x3D; 16384\nnet.ipv4.tcp_timestamps &#x3D; 0\nnet.core.somaxconn &#x3D; 16384\nEOF\nsysctl --system\n\n#所有节点配置完内核后，重启服务器，保证重启后内核依旧加载\nreboot -h now\n\n#重启后查看结果：\nlsmod | grep --color&#x3D;auto -e ip_vs -e nf_conntrack\n\n（三）高可用配置  （使用haproxy和keepalived部署高可用）安装keepalived和haproxyyum install keepalived haproxy -y\n\n配置haproxymv etc&#x2F;keepalived&#x2F;keepalived.conf etc&#x2F;keepalived&#x2F;keepalived.conf.bk\ncat &gt;&#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf&lt;&lt;&quot;EOF&quot;\n#---------------------------------------------------------------------\n# Example configuration for a possible web application.  See the\n# full configuration options online.\n#\n#   http:&#x2F;&#x2F;haproxy.1wt.eu&#x2F;download&#x2F;1.4&#x2F;doc&#x2F;configuration.txt\n#\n#---------------------------------------------------------------------\n\n#---------------------------------------------------------------------\n# Global settings\n#---------------------------------------------------------------------\nglobal\n    # to have these messages end up in &#x2F;var&#x2F;log&#x2F;haproxy.log you will\n    # need to:\n    #\n    # 1) configure syslog to accept network log events.  This is done\n    #    by adding the &#39;-r&#39; option to the SYSLOGD_OPTIONS in\n    #    &#x2F;etc&#x2F;sysconfig&#x2F;syslog\n    #\n    # 2) configure local2 events to go to the &#x2F;var&#x2F;log&#x2F;haproxy.log\n    #   file. A line like the following can be added to\n    #   &#x2F;etc&#x2F;sysconfig&#x2F;syslog\n    #\n    #    local2.*                       &#x2F;var&#x2F;log&#x2F;haproxy.log\n    #\n    log         127.0.0.1 local2\n\n    chroot      &#x2F;var&#x2F;lib&#x2F;haproxy\n    pidfile     &#x2F;var&#x2F;run&#x2F;haproxy.pid\n    maxconn     4000\n    user        haproxy\n    group       haproxy\n    daemon\n\n    # turn on stats unix socket\n    stats socket &#x2F;var&#x2F;lib&#x2F;haproxy&#x2F;stats\n\n#---------------------------------------------------------------------\n# common defaults that all the &#39;listen&#39; and &#39;backend&#39; sections will\n# use if not designated in their block\n#---------------------------------------------------------------------\ndefaults\n    mode                    http\n    log                     global\n    option                  httplog\n    option                  dontlognull\n    option http-server-close\n    option                  redispatch\n    option                  tcplog\n    retries                 3\n    timeout http-request    10s\n    timeout queue           1m\n    timeout connect         10s\n    timeout client          1m\n    timeout server          1m\n    timeout http-keep-alive 10s\n    timeout check           10s\n    maxconn                 3000\n\n#---------------------------------------------------------------------\n# main frontend which proxys to the backends\n#---------------------------------------------------------------------\nfrontend  main *:5000\n    acl url_static       path_beg       -i &#x2F;static &#x2F;images &#x2F;javascript &#x2F;stylesheets\n    acl url_static       path_end       -i .jpg .gif .png .css .js\n\n    use_backend static          if url_static\n    default_backend             app\n\n\n#---------------------------------------------------------------------\n# static backend for serving up images, stylesheets and such\n#---------------------------------------------------------------------\nbackend static\n    balance     roundrobin\n    server      static 127.0.0.1:4331 check\n\n#---------------------------------------------------------------------\n# round robin balancing between the various backends\n#---------------------------------------------------------------------\nbackend app\n    balance     roundrobin\n    server  app1 127.0.0.1:5001 check\n    server  app2 127.0.0.1:5002 check\n    server  app3 127.0.0.1:5003 check\n    server  app4 127.0.0.1:5004 check\n\nlisten k8s-master\n  bind 127.0.0.1:16443\n  bind 0.0.0.0:16443\n  mode tcp\n  server master1 192.168.2.18:6443 check inter 3s fall 3 rise 3   #注意修改IP\n  server master2 192.168.2.19:6443 check inter 3s fall 3 rise 3\n  server master3 192.168.2.20:6443 check inter 3s fall 3 rise 3\nEOF\n\n配置keepalived每个master节点不一样 注意修改\ncat &gt;&#x2F;etc&#x2F;keepalived&#x2F;keepalived.conf&lt;&lt;&quot;EOF&quot;\n! Configuration File for keepalived\nglobal_defs &#123;\n   router_id LVS_DEVEL\nscript_user root\n   enable_script_security\n&#125;\nvrrp_script chk_apiserver &#123;\n   script &quot;&#x2F;etc&#x2F;keepalived&#x2F;check_apiserver.sh&quot;\n   interval 5\n   weight -5\n   fall 2 \nrise 1\n&#125;\nvrrp_instance VI_1 &#123;\n   state MASTER\n   interface ens33 #注意网卡名字\n   mcast_src_ip 192.168.2.18 #注意修改master节点IP\n   virtual_router_id 51\n   priority 100\n   advert_int 2\n   authentication &#123;\n       auth_type PASS\n       auth_pass K8SHA_KA_AUTH\n   &#125;\n   virtual_ipaddress &#123;\n       192.168.2.188 #此处为VIP 如果不是高可用架构 修改为master1的IP\n   &#125;\n   track_script &#123;\n      chk_apiserver\n   &#125;\n&#125;\nEOF\n\n健康检查脚本cat &gt; &#x2F;etc&#x2F;keepalived&#x2F;check_apiserver.sh &lt;&lt;&quot;EOF&quot;\n#!&#x2F;bin&#x2F;bash\nerr&#x3D;0\nfor k in $(seq 1 3)\ndo\n   check_code&#x3D;$(pgrep haproxy)\n   if [[ $check_code &#x3D;&#x3D; &quot;&quot; ]]; then\n       err&#x3D;$(expr $err + 1)\n       sleep 1\n       continue\n   else\n       err&#x3D;0\n       break\n   fi\ndone\n\nif [[ $err !&#x3D; &quot;0&quot; ]]; then\n   echo &quot;systemctl stop keepalived&quot;\n   &#x2F;usr&#x2F;bin&#x2F;systemctl stop keepalived\n   exit 1\nelse\n   exit 0\nfi\nEOF\n\nchmod u+x &#x2F;etc&#x2F;keepalived&#x2F;check_apiserver.sh\n\n启动服务systemctl daemon-reload\nsystemctl enable --now haproxy\nsystemctl enable --now keepalived\n\n检查VIP是否正常配上\n#master01，看到vip\nip addr\n\n#各节点测试\nsystemctl status keepalived haproxy \nping 192.168.2.188 \ntelnet  192.168.2.188  16443\n\n（四）搭建etcd集群配置工作目录mkdir -p &#x2F;data&#x2F;ssl&#x2F;etc\nmkdir -p &#x2F;data&#x2F;ssl&#x2F;k8s\n\n生成cfss证书安装cfssl工具cd &#x2F;data\nwget https:&#x2F;&#x2F;pkg.cfssl.org&#x2F;R1.2&#x2F;cfssl_linux-amd64\nwget https:&#x2F;&#x2F;pkg.cfssl.org&#x2F;R1.2&#x2F;cfssljson_linux-amd64\nwget https:&#x2F;&#x2F;pkg.cfssl.org&#x2F;R1.2&#x2F;cfssl-certinfo_linux-amd64\n\nchmod +x cfssl*\nmv cfssl_linux-amd64 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;cfssl\nmv cfssljson_linux-amd64 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;cfssljson\nmv cfssl-certinfo_linux-amd64 &#x2F;usr&#x2F;local&#x2F;bin&#x2F;cfssl-certinfo\n\n配置ca请求文件cat &gt; ca-csr.json &lt;&lt;&quot;EOF&quot;\n&#123;\n  &quot;CN&quot;: &quot;kubernetes&quot;,\n  &quot;key&quot;: &#123;\n      &quot;algo&quot;: &quot;rsa&quot;,\n      &quot;size&quot;: 2048\n  &#125;,\n  &quot;names&quot;: [\n    &#123;\n      &quot;C&quot;: &quot;CN&quot;,\n      &quot;ST&quot;: &quot;Guangdong&quot;,\n      &quot;L&quot;: &quot;shenzhen&quot;,\n      &quot;O&quot;: &quot;k8s&quot;,\n      &quot;OU&quot;: &quot;system&quot;\n    &#125;\n  ],\n  &quot;ca&quot;: &#123;\n          &quot;expiry&quot;: &quot;87600h&quot;\n  &#125;\n&#125;\nEOF\n\n创建ca证书cfssl gencert -initca ca-csr.json | cfssljson -bare ca\n\n配置ca证书策略cat &gt; ca-config.json &lt;&lt;&quot;EOF&quot;\n&#123;\n  &quot;signing&quot;: &#123;\n      &quot;default&quot;: &#123;\n          &quot;expiry&quot;: &quot;87600h&quot;\n        &#125;,\n      &quot;profiles&quot;: &#123;\n          &quot;kubernetes&quot;: &#123;\n              &quot;usages&quot;: [\n                  &quot;signing&quot;,\n                  &quot;key encipherment&quot;,\n                  &quot;server auth&quot;,\n                  &quot;client auth&quot;\n              ],\n              &quot;expiry&quot;: &quot;87600h&quot;\n          &#125;\n      &#125;\n  &#125;\n&#125;\nEOF\n\n配置etcd请求csr文件cat &gt; etcd-csr.json &lt;&lt;&quot;EOF&quot;\n&#123;\n  &quot;CN&quot;: &quot;etcd&quot;,\n  &quot;hosts&quot;: [\n    &quot;127.0.0.1&quot;,\n    &quot;192.168.2.18&quot;, #注意修改IP为master IP\n    &quot;192.168.2.19&quot;,\n    &quot;192.168.2.20&quot;\n  ],\n  &quot;key&quot;: &#123;\n    &quot;algo&quot;: &quot;rsa&quot;,\n    &quot;size&quot;: 2048\n  &#125;,\n  &quot;names&quot;: [&#123;\n    &quot;C&quot;: &quot;CN&quot;,\n    &quot;ST&quot;: &quot;Guangdong&quot;,\n    &quot;L&quot;: &quot;shenzhen&quot;,\n    &quot;O&quot;: &quot;k8s&quot;,\n    &quot;OU&quot;: &quot;system&quot;\n  &#125;]\n&#125;\nEOF\n\n生成证书cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;kubernetes etcd-csr.json | cfssljson  -bare etcd\n\nls etcd*.pem\n# etcd-key.pem  etcd.pem\n\n下载etcdwget https:&#x2F;&#x2F;github.com&#x2F;etcd-io&#x2F;etcd&#x2F;releases&#x2F;download&#x2F;v3.5.0&#x2F;etcd-v3.5.0-linux-amd64.tar.gz\ntar -xvf etcd-v3.5.0-linux-amd64.tar.gz\ncp -p etcd-v3.5.0-linux-amd64&#x2F;etcd* &#x2F;usr&#x2F;local&#x2F;bin&#x2F;\nscp  etcd-v3.5.0-linux-amd64&#x2F;etcd* master2:&#x2F;usr&#x2F;local&#x2F;bin&#x2F;\nscp  etcd-v3.5.0-linux-amd64&#x2F;etcd* master2:&#x2F;usr&#x2F;local&#x2F;bin\n\n创建配置文件cat &gt;  etcd.conf &lt;&lt;&quot;EOF&quot;\n#[Member]\nETCD_NAME&#x3D;&quot;etcd1&quot;\nETCD_DATA_DIR&#x3D;&quot;&#x2F;var&#x2F;lib&#x2F;etcd&#x2F;default.etcd&quot;\nETCD_LISTEN_PEER_URLS&#x3D;&quot;https:&#x2F;&#x2F;192.168.2.18:2380&quot;  #注意修改IP\nETCD_LISTEN_CLIENT_URLS&#x3D;&quot;https:&#x2F;&#x2F;192.168.2.18:2379,http:&#x2F;&#x2F;127.0.0.1:2379&quot;\n\n#[Clustering]\nETCD_INITIAL_ADVERTISE_PEER_URLS&#x3D;&quot;https:&#x2F;&#x2F;192.168.2.18:2380&quot;  \nETCD_ADVERTISE_CLIENT_URLS&#x3D;&quot;https:192.168.2.18&#x2F;&#x2F;:2379&quot;\nETCD_INITIAL_CLUSTER&#x3D;&quot;etcd1&#x3D;https:&#x2F;&#x2F;192.168.2.18:2380,etcd2&#x3D;https:&#x2F;&#x2F;192.168.2.19:2380,etcd3&#x3D;https:&#x2F;&#x2F;192.168.2.20:2380&quot;\nETCD_INITIAL_CLUSTER_TOKEN&#x3D;&quot;etcd-cluster&quot;\nETCD_INITIAL_CLUSTER_STATE&#x3D;&quot;new&quot;\nEOF\n\n创建启动文件cat &gt; etcd.service &lt;&lt;&quot;EOF&quot;\n[Unit]\nDescription&#x3D;Etcd Server\nAfter&#x3D;network.target\nAfter&#x3D;network-online.target\nWants&#x3D;network-online.target\n\n[Service]\nType&#x3D;notify\nEnvironmentFile&#x3D;-&#x2F;etc&#x2F;etcd&#x2F;etcd.conf\nWorkingDirectory&#x3D;&#x2F;var&#x2F;lib&#x2F;etcd&#x2F;\nExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;etcd \\\n  --cert-file&#x3D;&#x2F;etc&#x2F;etcd&#x2F;ssl&#x2F;etcd.pem \\\n  --key-file&#x3D;&#x2F;etc&#x2F;etcd&#x2F;ssl&#x2F;etcd-key.pem \\\n  --trusted-ca-file&#x3D;&#x2F;etc&#x2F;etcd&#x2F;ssl&#x2F;ca.pem \\\n  --peer-cert-file&#x3D;&#x2F;etc&#x2F;etcd&#x2F;ssl&#x2F;etcd.pem \\\n  --peer-key-file&#x3D;&#x2F;etc&#x2F;etcd&#x2F;ssl&#x2F;etcd-key.pem \\\n  --peer-trusted-ca-file&#x3D;&#x2F;etc&#x2F;etcd&#x2F;ssl&#x2F;ca.pem \\\n  --peer-client-cert-auth \\\n  --client-cert-auth\nRestart&#x3D;on-failure\nRestartSec&#x3D;5\nLimitNOFILE&#x3D;65536\n\n[Install]\nWantedBy&#x3D;multi-user.target\nEOF\n\nmaster节点创建目录mkdir -p &#x2F;etc&#x2F;etcd\nmkdir -p &#x2F;etc&#x2F;etcd&#x2F;ssl\nmkdir -p &#x2F;var&#x2F;lib&#x2F;etcd&#x2F;default.etcd\n\n同步各个master节点cp ca*.pem &#x2F;etc&#x2F;etcd&#x2F;ssl&#x2F;\ncp etcd*.pem &#x2F;etc&#x2F;etcd&#x2F;ssl&#x2F;\ncp etcd.conf &#x2F;etc&#x2F;etcd&#x2F;\ncp etcd.service &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;\nfor i in master2 master3;do scp  etcd.conf $i:&#x2F;etc&#x2F;etcd&#x2F;;done\nfor i in master2 master3;do scp  etcd*.pem ca*.pem $i:&#x2F;etc&#x2F;etcd&#x2F;ssl&#x2F;;done\nfor i in master2 master3;do scp  etcd.service $i:&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;;done\n\nmaster2和master3修改IPvim &#x2F;etc&#x2F;etcd&#x2F;etcd.conf\n\n启动etcd集群systemctl daemon-reload\nsystemctl enable --now etcd.service\nsystemctl status etcd\n\n查看集群状态#注意修改IP\nETCD_API&#x3D;3 etcdctl --write-out&#x3D;table \\\n--cacert&#x3D;&#x2F;etc&#x2F;etcd&#x2F;ssl&#x2F;ca.pem \\\n--cert&#x3D;&#x2F;etc&#x2F;etcd&#x2F;ssl&#x2F;etcd.pem \\\n--key&#x3D;&#x2F;etc&#x2F;etcd&#x2F;ssl&#x2F;etcd-key.pem \\\n--endpoints&#x3D;https:&#x2F;&#x2F;192.168.2.18:2379,https:&#x2F;&#x2F;192.168.2.19:2379,https:&#x2F;&#x2F;192.168.2.20:2379 endpoint health\n\n（五）kuberenets组件配置安装Docker下载安装包wget https:&#x2F;&#x2F;download.docker.com&#x2F;linux&#x2F;static&#x2F;stable&#x2F;x86_64&#x2F;docker-20.10.9.tgz\n\n解压安装包tar -xvf docker-20.10.9.tgz\n\n复制文件cp docker&#x2F;* &#x2F;usr&#x2F;bin\n\n配置docker启动文件cat &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service &gt;&gt; EOF\n[Unit]\nDescription&#x3D;Docker Application Container Engine\nDocumentation&#x3D;https:&#x2F;&#x2F;docs.docker.com\nAfter&#x3D;network-online.target firewalld.service\nWants&#x3D;network-online.target\n\n[Service]\nType&#x3D;notify\n# the default is not to use systemd for cgroups because the delegate issues still\n# exists and systemd currently does not support the cgroup feature set required\n# for containers run by docker\nExecStart&#x3D;&#x2F;usr&#x2F;bin&#x2F;dockerd --selinux-enabled&#x3D;false --insecure-registry&#x3D;192.168.3.10\nExecReload&#x3D;&#x2F;bin&#x2F;kill -s HUP $MAINPID\n# Having non-zero Limit*s causes performance problems due to accounting overhead\n# in the kernel. We recommend using cgroups to do container-local accounting.\nLimitNOFILE&#x3D;infinity\nLimitNPROC&#x3D;infinity\nLimitCORE&#x3D;infinity\n# Uncomment TasksMax if your systemd version supports it.\n# Only systemd 226 and above support this version.\n#TasksMax&#x3D;infinity\nTimeoutStartSec&#x3D;0\n# set delegate yes so that systemd does not reset the cgroups of docker containers\nDelegate&#x3D;yes\n# kill only the docker process, not all processes in the cgroup\nKillMode&#x3D;process\n# restart the docker process if it exits prematurely\nRestart&#x3D;on-failure\nStartLimitBurst&#x3D;3\nStartLimitInterval&#x3D;60s\n\n[Install]\nWantedBy&#x3D;multi-user.target\nEOF\n\nchmod 777 &#x2F;etc&#x2F;systemd&#x2F;system&#x2F;docker.service \n\n启动dockersystemctl daemon-reload\nsystemctl start docker\nsystemctl enable docker\nsystemctl status docker\n\n下载安装包wget https:&#x2F;&#x2F;dl.k8s.io&#x2F;v1.22.1&#x2F;kubernetes-server-linux-amd64.tar.gz\ntar -xvf kubernetes-server-linux-amd64.tar.gz\ncd kubernetes&#x2F;server&#x2F;bin&#x2F;\ncp kube-apiserver kube-controller-manager kube-scheduler kubectl &#x2F;usr&#x2F;local&#x2F;bin&#x2F;\nscp   kube-apiserver kube-controller-manager kube-scheduler kubectl master2:&#x2F;usr&#x2F;local&#x2F;bin&#x2F;\nscp   kube-apiserver kube-controller-manager kube-scheduler kubectl master3:&#x2F;usr&#x2F;local&#x2F;bin&#x2F;\nfor i in node1 node2.px-k8s ;do scp  kubelet kube-proxy $i:&#x2F;usr&#x2F;local&#x2F;bin&#x2F;;done\n\n所有节点创建工作目录mkdir -p &#x2F;etc&#x2F;kubernetes&#x2F;      \nmkdir -p &#x2F;etc&#x2F;kubernetes&#x2F;ssl   \nmkdir -p &#x2F;var&#x2F;log&#x2F;kubernetes      \n\n部署api-servercat &gt; kube-apiserver-csr.json &lt;&lt; &quot;EOF&quot;\n&#123;\n&quot;CN&quot;: &quot;kubernetes&quot;,\n  &quot;hosts&quot;: [\n    &quot;127.0.0.1&quot;,\n    &quot;192.168.2.188&quot;, #这个是VIP的IP\n    &quot;192.168.2.18&quot;,  #修改master的IP\n    &quot;192.168.2.19&quot;,\n    &quot;192.168.2.20&quot;,\n    &quot;192.168.2.21&quot;,\n    &quot;192.168.2.22&quot;,\n    &quot;192.168.2.23&quot;,\n    &quot;10.96.0.1&quot;,\n    &quot;kubernetes&quot;,\n    &quot;kubernetes.default&quot;,\n    &quot;kubernetes.default.svc&quot;,\n    &quot;kubernetes.default.svc.cluster&quot;,\n    &quot;kubernetes.default.svc.cluster.local&quot;\n  ],\n  &quot;key&quot;: &#123;\n    &quot;algo&quot;: &quot;rsa&quot;,\n    &quot;size&quot;: 2048\n  &#125;,\n  &quot;names&quot;: [\n    &#123;\n      &quot;C&quot;: &quot;CN&quot;,\n      &quot;ST&quot;: &quot;Guangdong&quot;,\n      &quot;L&quot;: &quot;shenzhen&quot;,\n      &quot;O&quot;: &quot;k8s&quot;,\n      &quot;OU&quot;: &quot;system&quot;\n    &#125;\n  ]\n&#125;\nEOF\n\n生成证书和token文件cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;kubernetes kube-apiserver-csr.json | cfssljson -bare kube-apiserver\n\ncat &gt; token.csv &lt;&lt; EOF\n$(head -c 16 &#x2F;dev&#x2F;urandom | od -An -t x | tr -d &#39; &#39;),kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;\nEOF\n\n创建配置文件cat &gt; kube-apiserver.conf &lt;&lt; &quot;EOF&quot;\nKUBE_APISERVER_OPTS&#x3D;&quot;--enable-admission-plugins&#x3D;NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\\n  --anonymous-auth&#x3D;false \\\n  --bind-address&#x3D;192.168.2.18 \\\n  --secure-port&#x3D;6443 \\\n  --advertise-address&#x3D;192.168.2.18 \\\n  --insecure-port&#x3D;0 \\\n  --authorization-mode&#x3D;Node,RBAC \\\n  --runtime-config&#x3D;api&#x2F;all&#x3D;true \\\n  --enable-bootstrap-token-auth \\\n  --service-cluster-ip-range&#x3D;10.96.0.0&#x2F;16 \\\n  --token-auth-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;token.csv \\\n  --service-node-port-range&#x3D;30000-50000 \\\n  --tls-cert-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;kube-apiserver.pem  \\\n  --tls-private-key-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;kube-apiserver-key.pem \\\n  --client-ca-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;ca.pem \\\n  --kubelet-client-certificate&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;kube-apiserver.pem \\\n  --kubelet-client-key&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;kube-apiserver-key.pem \\\n  --service-account-key-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;ca-key.pem \\\n  --service-account-signing-key-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;ca-key.pem  \\\n  --service-account-issuer&#x3D;api \\\n  --etcd-cafile&#x3D;&#x2F;etc&#x2F;etcd&#x2F;ssl&#x2F;ca.pem \\\n  --etcd-certfile&#x3D;&#x2F;etc&#x2F;etcd&#x2F;ssl&#x2F;etcd.pem \\\n  --etcd-keyfile&#x3D;&#x2F;etc&#x2F;etcd&#x2F;ssl&#x2F;etcd-key.pem \\\n  --etcd-servers&#x3D;https:&#x2F;&#x2F;192.168.2.18:2379,https:&#x2F;&#x2F;192.168.2.19:2379,https:&#x2F;&#x2F;192.168.2.20:2379 \\\n  --enable-swagger-ui&#x3D;true \\\n  --allow-privileged&#x3D;true \\\n  --apiserver-count&#x3D;3 \\\n  --audit-log-maxage&#x3D;30 \\\n  --audit-log-maxbackup&#x3D;3 \\\n  --audit-log-maxsize&#x3D;100 \\\n  --audit-log-path&#x3D;&#x2F;var&#x2F;log&#x2F;kube-apiserver-audit.log \\\n  --event-ttl&#x3D;1h \\\n  --alsologtostderr&#x3D;true \\\n  --logtostderr&#x3D;false \\\n  --log-dir&#x3D;&#x2F;var&#x2F;log&#x2F;kubernetes \\\n  --v&#x3D;4&quot;\nEOF\n\n创建apiserver文件cat &gt; kube-apiserver.service &lt;&lt; &quot;EOF&quot;\n[Unit]\nDescription&#x3D;Kubernetes API Server\nDocumentation&#x3D;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kubernetes\nAfter&#x3D;etcd.service\nWants&#x3D;etcd.service\n\n[Service]\nEnvironmentFile&#x3D;-&#x2F;etc&#x2F;kubernetes&#x2F;kube-apiserver.conf\nExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;kube-apiserver $KUBE_APISERVER_OPTS\nRestart&#x3D;on-failure\nRestartSec&#x3D;5\nType&#x3D;notify\nLimitNOFILE&#x3D;65536\n\n[Install]\nWantedBy&#x3D;multi-user.target\nEOF\n\n同步文件到节点上cp ca*.pem &#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;\ncp kube-apiserver*.pem &#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;\ncp token.csv &#x2F;etc&#x2F;kubernetes&#x2F;\ncp kube-apiserver.conf &#x2F;etc&#x2F;kubernetes&#x2F; \ncp kube-apiserver.service &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;\nscp   token.csv master2:&#x2F;etc&#x2F;kubernetes&#x2F;\nscp   token.csv master3:&#x2F;etc&#x2F;kubernetes&#x2F;\nscp   kube-apiserver*.pem master2:&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;\nscp   kube-apiserver*.pem master3:&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;\nscp   ca*.pem master2:&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;\nscp   ca*.pem master3:&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;\nscp   kube-apiserver.conf master2:&#x2F;etc&#x2F;kubernetes&#x2F;\nscp   kube-apiserver.conf master03.px-k8s:&#x2F;etc&#x2F;kubernetes&#x2F;\nscp   kube-apiserver.service master2:&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;\nscp   kube-apiserver.service master3:&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;\n\n修改master2和master3的IPvim &#x2F;etc&#x2F;kubernetes&#x2F;kube-apiserver.conf\n\n启动服务systemctl daemon-reload\nsystemctl enable --now kube-apiserver\nsystemctl status kube-apiserver\n\n# 测试\ncurl --insecure https:&#x2F;&#x2F;192.168.2.18:6443&#x2F;\ncurl --insecure https:&#x2F;&#x2F;192.168.2.19:6443&#x2F;\ncurl --insecure https:&#x2F;&#x2F;192.168.2.20:6443&#x2F;\ncurl --insecure https:&#x2F;&#x2F;192.168.2.188:16443&#x2F;\n\n部署kube-controller-manager创建csr请求文件cat &gt; kube-controller-manager-csr.json &lt;&lt; &quot;EOF&quot;\n&#123;\n    &quot;CN&quot;: &quot;system:kube-controller-manager&quot;,\n    &quot;key&quot;: &#123;\n        &quot;algo&quot;: &quot;rsa&quot;,\n        &quot;size&quot;: 2048\n    &#125;,\n    &quot;hosts&quot;: [\n      &quot;127.0.0.1&quot;,\n      &quot;192.168.2.18&quot;,\n      &quot;192.168.2.19&quot;,\n      &quot;192.168.2.20&quot;\n    ],\n    &quot;names&quot;: [\n      &#123;\n        &quot;C&quot;: &quot;CN&quot;,\n        &quot;ST&quot;: &quot;Guangdong&quot;,\n        &quot;L&quot;: &quot;shenzhen&quot;,\n        &quot;O&quot;: &quot;system:kube-controller-manager&quot;,\n        &quot;OU&quot;: &quot;system&quot;\n      &#125;\n    ]\n&#125;\nEOF\n\n生成证书cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager\n\nls kube-controller-manager*.pem\n\n创建kubeconfig配置文件kubectl config set-cluster kubernetes --certificate-authority&#x3D;ca.pem --embed-certs&#x3D;true --server&#x3D;https:&#x2F;&#x2F;192.168.2.188:16443 --kubeconfig&#x3D;kube-controller-manager.kubeconfig\nkubectl config set-credentials system:kube-controller-manager --client-certificate&#x3D;kube-controller-manager.pem --client-key&#x3D;kube-controller-manager-key.pem --embed-certs&#x3D;true --kubeconfig&#x3D;kube-controller-manager.kubeconfig\nkubectl config set-context system:kube-controller-manager --cluster&#x3D;kubernetes --user&#x3D;system:kube-controller-manager --kubeconfig&#x3D;kube-controller-manager.kubeconfig\nkubectl config use-context system:kube-controller-manager --kubeconfig&#x3D;kube-controller-manager.kubeconfig\n\n创建conf配置文件cat &gt; kube-controller-manager.conf &lt;&lt; &quot;EOF&quot;\nKUBE_CONTROLLER_MANAGER_OPTS&#x3D;&quot;--port&#x3D;0 \\\n  --secure-port&#x3D;10257 \\\n  --bind-address&#x3D;127.0.0.1 \\\n  --kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;kube-controller-manager.kubeconfig \\\n  --service-cluster-ip-range&#x3D;10.96.0.0&#x2F;16 \\\n  --cluster-name&#x3D;kubernetes \\\n  --cluster-signing-cert-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;ca.pem \\\n  --cluster-signing-key-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;ca-key.pem \\\n  --allocate-node-cidrs&#x3D;true \\\n  --cluster-cidr&#x3D;172.168.0.0&#x2F;16 \\\n  --experimental-cluster-signing-duration&#x3D;87600h \\\n  --root-ca-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;ca.pem \\\n  --service-account-private-key-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;ca-key.pem \\\n  --leader-elect&#x3D;true \\\n  --feature-gates&#x3D;RotateKubeletServerCertificate&#x3D;true \\\n  --controllers&#x3D;*,bootstrapsigner,tokencleaner \\\n  --horizontal-pod-autoscaler-sync-period&#x3D;10s \\\n  --tls-cert-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;kube-controller-manager.pem \\\n  --tls-private-key-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;kube-controller-manager-key.pem \\\n  --use-service-account-credentials&#x3D;true \\\n  --alsologtostderr&#x3D;true \\\n  --logtostderr&#x3D;false \\\n  --log-dir&#x3D;&#x2F;var&#x2F;log&#x2F;kubernetes \\\n  --v&#x3D;2&quot;\nEOF\n\n创建启动文件cat &gt; kube-controller-manager.service &lt;&lt; &quot;EOF&quot;\n[Unit]\nDescription&#x3D;Kubernetes Controller Manager\nDocumentation&#x3D;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kubernetes\n\n[Service]\nEnvironmentFile&#x3D;-&#x2F;etc&#x2F;kubernetes&#x2F;kube-controller-manager.conf\nExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;kube-controller-manager $KUBE_CONTROLLER_MANAGER_OPTS\nRestart&#x3D;on-failure\nRestartSec&#x3D;5\n\n[Install]\nWantedBy&#x3D;multi-user.target\nEOF\n\n同步相关文件到各个节点cp kube-controller-manager*.pem &#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;\ncp kube-controller-manager.kubeconfig &#x2F;etc&#x2F;kubernetes&#x2F;\ncp kube-controller-manager.conf &#x2F;etc&#x2F;kubernetes&#x2F;\ncp kube-controller-manager.service &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;\nscp  kube-controller-manager*.pem master2:&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;\nscp  kube-controller-manager*.pem master3:&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;\nscp  kube-controller-manager.kubeconfig kube-controller-manager.conf master2:&#x2F;etc&#x2F;kubernetes&#x2F;\nscp  kube-controller-manager.kubeconfig kube-controller-manager.conf master3:&#x2F;etc&#x2F;kubernetes&#x2F;\nscp  kube-controller-manager.service master2:&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;\nscp  kube-controller-manager.service master3:&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;\n\n#查看证书\nopenssl x509 -in &#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;kube-controller-manager.pem -noout -text\n\n启动服务systemctl daemon-reload \nsystemctl enable --now kube-controller-manager\nsystemctl status kube-controller-manager\n\n部署kube-scheduler创建csr请求文件cat &gt; kube-scheduler-csr.json &lt;&lt; &quot;EOF&quot;\n&#123;\n    &quot;CN&quot;: &quot;system:kube-scheduler&quot;,\n    &quot;hosts&quot;: [\n      &quot;127.0.0.1&quot;,\n      &quot;192.168.2.18&quot;,\n      &quot;192.168.2.19&quot;,\n      &quot;192.168.2.20&quot;\n    ],\n    &quot;key&quot;: &#123;\n        &quot;algo&quot;: &quot;rsa&quot;,\n        &quot;size&quot;: 2048\n    &#125;,\n    &quot;names&quot;: [\n      &#123;\n        &quot;C&quot;: &quot;CN&quot;,\n        &quot;ST&quot;: &quot;Guangdong&quot;,\n        &quot;L&quot;: &quot;shenzhen&quot;,\n        &quot;O&quot;: &quot;system:kube-scheduler&quot;,\n        &quot;OU&quot;: &quot;system&quot;\n      &#125;\n    ]\n&#125;\nEOF\n\n生成证书cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler\n\nls kube-scheduler*.pem\n\n创建kubeconfigkubectl config set-cluster kubernetes --certificate-authority&#x3D;ca.pem --embed-certs&#x3D;true --server&#x3D;https:&#x2F;&#x2F;192.168.2.188:16443 --kubeconfig&#x3D;kube-scheduler.kubeconfig\nkubectl config set-credentials system:kube-scheduler --client-certificate&#x3D;kube-scheduler.pem --client-key&#x3D;kube-scheduler-key.pem --embed-certs&#x3D;true --kubeconfig&#x3D;kube-scheduler.kubeconfig\nkubectl config set-context system:kube-scheduler --cluster&#x3D;kubernetes --user&#x3D;system:kube-scheduler --kubeconfig&#x3D;kube-scheduler.kubeconfig\nkubectl config use-context system:kube-scheduler --kubeconfig&#x3D;kube-scheduler.kubeconfig\n\n创建配置文件cat &gt; kube-scheduler.conf &lt;&lt; &quot;EOF&quot;\nKUBE_SCHEDULER_OPTS&#x3D;&quot;--address&#x3D;127.0.0.1 \\\n--kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;kube-scheduler.kubeconfig \\\n--leader-elect&#x3D;true \\\n--alsologtostderr&#x3D;true \\\n--logtostderr&#x3D;false \\\n--log-dir&#x3D;&#x2F;var&#x2F;log&#x2F;kubernetes \\\n--v&#x3D;2&quot;\nEOF\n\n创建启动文件cat &gt; kube-scheduler.service &lt;&lt; &quot;EOF&quot;\n[Unit]\nDescription&#x3D;Kubernetes Scheduler\nDocumentation&#x3D;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kubernetes\n\n[Service]\nEnvironmentFile&#x3D;-&#x2F;etc&#x2F;kubernetes&#x2F;kube-scheduler.conf\nExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;kube-scheduler $KUBE_SCHEDULER_OPTS\nRestart&#x3D;on-failure\nRestartSec&#x3D;5\n\n[Install]\nWantedBy&#x3D;multi-user.target\nEOF\n\n同步文件cp kube-scheduler*.pem &#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;\ncp kube-scheduler.kubeconfig &#x2F;etc&#x2F;kubernetes&#x2F;\ncp kube-scheduler.conf &#x2F;etc&#x2F;kubernetes&#x2F;\ncp kube-scheduler.service &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;\nscp  kube-scheduler*.pem master2:&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;\nscp  kube-scheduler*.pem master3:&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;\nscp  kube-scheduler.kubeconfig kube-scheduler.conf master2-k8s:&#x2F;etc&#x2F;kubernetes&#x2F;\nscp  kube-scheduler.kubeconfig kube-scheduler.conf master3:&#x2F;etc&#x2F;kubernetes&#x2F;\nscp  kube-scheduler.service master2:&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;\nscp  kube-scheduler.service master3:&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;\n\n启动服务systemctl daemon-reload\nsystemctl enable --now kube-scheduler\nsystemctl status kube-scheduler\n\n部署kubectl创建csr请求文件cat &gt; admin-csr.json &lt;&lt; &quot;EOF&quot;\n&#123;\n  &quot;CN&quot;: &quot;admin&quot;,\n  &quot;hosts&quot;: [],\n  &quot;key&quot;: &#123;\n    &quot;algo&quot;: &quot;rsa&quot;,\n    &quot;size&quot;: 2048\n  &#125;,\n  &quot;names&quot;: [\n    &#123;\n      &quot;C&quot;: &quot;CN&quot;,\n      &quot;ST&quot;: &quot;Guangdong&quot;,\n      &quot;L&quot;: &quot;shenzhen&quot;,\n      &quot;O&quot;: &quot;system:masters&quot;,           \n      &quot;OU&quot;: &quot;system&quot;\n    &#125;\n  ]\n&#125;\nEOF\n\n生成证书cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;kubernetes admin-csr.json | cfssljson -bare admin\n\ncp admin*.pem &#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;\n\nkubeconfig配置#记得修改IP\nkubectl config set-cluster kubernetes --certificate-authority&#x3D;ca.pem --embed-certs&#x3D;true --server&#x3D;https:&#x2F;&#x2F;192.168.2.18:16443 --kubeconfig&#x3D;kube.config\nkubectl config set-credentials admin --client-certificate&#x3D;admin.pem --client-key&#x3D;admin-key.pem --embed-certs&#x3D;true --kubeconfig&#x3D;kube.config\nkubectl config set-context kubernetes --cluster&#x3D;kubernetes --user&#x3D;admin --kubeconfig&#x3D;kube.config\nkubectl config use-context kubernetes --kubeconfig&#x3D;kube.config\n\nmkdir ~&#x2F;.kube\ncp kube.config ~&#x2F;.kube&#x2F;config\nkubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole&#x3D;system:kubelet-api-admin --user kubernetes --kubeconfig&#x3D;~&#x2F;.kube&#x2F;config\n\n查看集群状态export KUBECONFIG&#x3D;$HOME&#x2F;.kube&#x2F;config\n\nkubectl cluster-info\nkubectl get componentstatuses\nkubectl get all --all-namespaces\n\n同步文件到其他节点scp    &#x2F;root&#x2F;.kube&#x2F;config master2:&#x2F;root&#x2F;.kube&#x2F;\nscp    &#x2F;root&#x2F;.kube&#x2F;config master3:&#x2F;root&#x2F;.kube&#x2F;\n\n配置kubectl命令补全yum install -y bash-completion\nsource &#x2F;usr&#x2F;share&#x2F;bash-completion&#x2F;bash_completion\nsource &lt;(kubectl completion bash)\nkubectl completion bash &gt; ~&#x2F;.kube&#x2F;completion.bash.inc\nsource &#39;&#x2F;root&#x2F;.kube&#x2F;completion.bash.inc&#39;  \nsource $HOME&#x2F;.bash_profile\n\n（六）TLS  Bootstrapping 配置在master1创建bootstrapcd &#x2F;root&#x2F;k8s-ha-install&#x2F;bootstrap\nkubectl config set-cluster kubernetes     --certificate-authority&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;ca.pem     --embed-certs&#x3D;true     --server&#x3D;https:&#x2F;&#x2F;192.168.2.188:16443     --kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;bootstrap-kubelet.kubeconfig\nkubectl config set-credentials tls-bootstrap-token-user     --token&#x3D;c8ad9c.2e4d610cf3e7426e --kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;bootstrap-kubelet.kubeconfig\nkubectl config set-context tls-bootstrap-token-user@kubernetes     --cluster&#x3D;kubernetes     --user&#x3D;tls-bootstrap-token-user     --kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;bootstrap-kubelet.kubeconfig\nkubectl config use-context tls-bootstrap-token-user@kubernetes     --kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;bootstrap-kubelet.kubeconfig\n\n创建yaml[root@k8s-master01 bootstrap]# cat bootstrap.secret.yaml \napiVersion: v1\nkind: Secret\nmetadata:\n  name: bootstrap-token-c8ad9c\n  namespace: kube-system\ntype: bootstrap.kubernetes.io&#x2F;token\nstringData:\n  description: &quot;The default bootstrap token generated by &#39;kubelet &#39;.&quot;\n  token-id: c8ad9c\n  token-secret: 2e4d610cf3e7426e\n  usage-bootstrap-authentication: &quot;true&quot;\n  usage-bootstrap-signing: &quot;true&quot;\n  auth-extra-groups:  system:bootstrappers:default-node-token,system:bootstrappers:worker,system:bootstrappers:ingress\n\n---\napiVersion: rbac.authorization.k8s.io&#x2F;v1\nkind: ClusterRoleBinding\nmetadata:\n  name: kubelet-bootstrap\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:node-bootstrapper\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:bootstrappers:default-node-token\n---\napiVersion: rbac.authorization.k8s.io&#x2F;v1\nkind: ClusterRoleBinding\nmetadata:\n  name: node-autoapprove-bootstrap\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:certificates.k8s.io:certificatesigningrequests:nodeclient\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:bootstrappers:default-node-token\n---\napiVersion: rbac.authorization.k8s.io&#x2F;v1\nkind: ClusterRoleBinding\nmetadata:\n  name: node-autoapprove-certificate-rotation\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient\nsubjects:\n- apiGroup: rbac.authorization.k8s.io\n  kind: Group\n  name: system:nodes\n---\napiVersion: rbac.authorization.k8s.io&#x2F;v1\nkind: ClusterRole\nmetadata:\n  annotations:\n    rbac.authorization.kubernetes.io&#x2F;autoupdate: &quot;true&quot;\n  labels:\n    kubernetes.io&#x2F;bootstrapping: rbac-defaults\n  name: system:kube-apiserver-to-kubelet\nrules:\n  - apiGroups:\n      - &quot;&quot;\n    resources:\n      - nodes&#x2F;proxy\n      - nodes&#x2F;stats\n      - nodes&#x2F;log\n      - nodes&#x2F;spec\n      - nodes&#x2F;metrics\n    verbs:\n      - &quot;*&quot;\n---\napiVersion: rbac.authorization.k8s.io&#x2F;v1\nkind: ClusterRoleBinding\nmetadata:\n  name: system:kube-apiserver\n  namespace: &quot;&quot;\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:kube-apiserver-to-kubelet\nsubjects:\n  - apiGroup: rbac.authorization.k8s.io\n    kind: User\n    name: kube-apiserver\n\n启动bootstrapmkdir -p &#x2F;root&#x2F;.kube ; cp &#x2F;etc&#x2F;kubernetes&#x2F;admin.kubeconfig &#x2F;root&#x2F;.kube&#x2F;config\nkubectl create -f bootstrap.secret.yaml \n\n（七）node节点部署部署kubelet在master1节点上执行BOOTSTRAP_TOKEN&#x3D;$(awk -F &quot;,&quot; &#39;&#123;print $1&#125;&#39; &#x2F;etc&#x2F;kubernetes&#x2F;token.csv)\nkubectl config set-cluster kubernetes --certificate-authority&#x3D;ca.pem --embed-certs&#x3D;true --server&#x3D;https:&#x2F;&#x2F;192.168.2.188:16443 --kubeconfig&#x3D;kubelet-bootstrap.kubeconfig\nkubectl config set-credentials kubelet-bootstrap --token&#x3D;$&#123;BOOTSTRAP_TOKEN&#125; --kubeconfig&#x3D;kubelet-bootstrap.kubeconfig\nkubectl config set-context default --cluster&#x3D;kubernetes --user&#x3D;kubelet-bootstrap --kubeconfig&#x3D;kubelet-bootstrap.kubeconfig\nkubectl config use-context default --kubeconfig&#x3D;kubelet-bootstrap.kubeconfig\nkubectl create clusterrolebinding kubelet-bootstrap --clusterrole&#x3D;system:node-bootstrapper --user&#x3D;kubelet-bootstrap --kubeconfig&#x3D;&#x2F;root&#x2F;.kube&#x2F;config\n\n创建配置文件cat &gt; kubelet.json &lt;&lt; &quot;EOF&quot;\n&#123;\n  &quot;kind&quot;: &quot;KubeletConfiguration&quot;,\n  &quot;apiVersion&quot;: &quot;kubelet.config.k8s.io&#x2F;v1beta1&quot;,\n  &quot;authentication&quot;: &#123;\n    &quot;x509&quot;: &#123;\n      &quot;clientCAFile&quot;: &quot;&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;ca.pem&quot;\n    &#125;,\n    &quot;webhook&quot;: &#123;\n      &quot;enabled&quot;: true,\n      &quot;cacheTTL&quot;: &quot;2m0s&quot;\n    &#125;,\n    &quot;anonymous&quot;: &#123;\n      &quot;enabled&quot;: false\n    &#125;\n  &#125;,\n  &quot;authorization&quot;: &#123;\n    &quot;mode&quot;: &quot;Webhook&quot;,\n    &quot;webhook&quot;: &#123;\n      &quot;cacheAuthorizedTTL&quot;: &quot;5m0s&quot;,\n      &quot;cacheUnauthorizedTTL&quot;: &quot;30s&quot;\n    &#125;\n  &#125;,\n  &quot;address&quot;: &quot;192.168.2.18&quot;,\n  &quot;port&quot;: 10250,\n  &quot;readOnlyPort&quot;: 10255,\n  &quot;cgroupDriver&quot;: &quot;systemd&quot;,                  \n  &quot;hairpinMode&quot;: &quot;promiscuous-bridge&quot;,\n  &quot;serializeImagePulls&quot;: false,\n  &quot;clusterDomain&quot;: &quot;cluster.local.&quot;,\n  &quot;clusterDNS&quot;: [&quot;10.96.0.2&quot;]\n&#125;\nEOF\n\n创建启动文件cat &gt; kubelet.service &lt;&lt; &quot;EOF&quot;\n[Unit]\nDescription&#x3D;Kubernetes Kubelet\nDocumentation&#x3D;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kubernetes\nAfter&#x3D;docker.service\nRequires&#x3D;docker.service\n\n[Service]\nWorkingDirectory&#x3D;&#x2F;var&#x2F;lib&#x2F;kubelet\nExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;kubelet \\\n  --bootstrap-kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;kubelet-bootstrap.kubeconfig \\\n  --cert-dir&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;ssl \\\n  --kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;kubelet.kubeconfig \\\n  --config&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;kubelet.json \\\n  --network-plugin&#x3D;cni \\\n  --rotate-certificates \\\n  --pod-infra-container-image&#x3D;registry.aliyuncs.com&#x2F;google_containers&#x2F;pause:3.2 \\\n  --alsologtostderr&#x3D;true \\\n  --logtostderr&#x3D;false \\\n  --log-dir&#x3D;&#x2F;var&#x2F;log&#x2F;kubernetes \\\n  --v&#x3D;2\nRestart&#x3D;on-failure\nRestartSec&#x3D;5\n\n[Install]\nWantedBy&#x3D;multi-user.target\nEOF\n\n同步文件cp kubelet-bootstrap.kubeconfig &#x2F;etc&#x2F;kubernetes&#x2F;\ncp kubelet.json &#x2F;etc&#x2F;kubernetes&#x2F;\ncp kubelet.service &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;\n\nfor i in  master2 master3 node1 node2 node3;do scp  kubelet-bootstrap.kubeconfig kubelet.json $i:&#x2F;etc&#x2F;kubernetes&#x2F;;done\n\nfor i in  master2 master3 node1 node2 node3;do scp  ca.pem $i:&#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;;done\n\nfor i in master2 master3 node1 node2 node3;do scp  kubelet.service $i:&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;;done\n\n所有节点执行mkdir -p &#x2F;var&#x2F;lib&#x2F;kubelet\nmkdir -p &#x2F;var&#x2F;log&#x2F;kubernetes\nsystemctl daemon-reload\nsystemctl enable --now kubelet\n\nsystemctl status kubelet\n\n节点加入集群kubectl get csr | grep Pending | awk &#39;&#123;print $1&#125;&#39; | xargs kubectl certificate approve\nkubectl get nodes\n\n部署kube-proxy创建csr请求文件cat &gt; kube-proxy-csr.json &lt;&lt; &quot;EOF&quot;\n&#123;\n  &quot;CN&quot;: &quot;system:kube-proxy&quot;,\n  &quot;key&quot;: &#123;\n    &quot;algo&quot;: &quot;rsa&quot;,\n    &quot;size&quot;: 2048\n  &#125;,\n  &quot;names&quot;: [\n    &#123;\n      &quot;C&quot;: &quot;CN&quot;,\n      &quot;ST&quot;: &quot;Guangdong&quot;,\n      &quot;L&quot;: &quot;shenzhen&quot;,\n      &quot;O&quot;: &quot;k8s&quot;,\n      &quot;OU&quot;: &quot;system&quot;\n    &#125;\n  ]\n&#125;\nEOF\n\n生成证书cfssl gencert -ca&#x3D;ca.pem -ca-key&#x3D;ca-key.pem -config&#x3D;ca-config.json -profile&#x3D;kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy\nls kube-proxy*.pem\n\n创建config文件kubectl config set-cluster kubernetes --certificate-authority&#x3D;ca.pem --embed-certs&#x3D;true --server&#x3D;https:&#x2F;&#x2F;192.168.2.188:16443 --kubeconfig&#x3D;kube-proxy.kubeconfig\n\nkubectl config set-credentials kube-proxy --client-certificate&#x3D;kube-proxy.pem --client-key&#x3D;kube-proxy-key.pem --embed-certs&#x3D;true --kubeconfig&#x3D;kube-proxy.kubeconfig\n\nkubectl config set-context default --cluster&#x3D;kubernetes --user&#x3D;kube-proxy --kubeconfig&#x3D;kube-proxy.kubeconfig\n\nkubectl config use-context default --kubeconfig&#x3D;kube-proxy.kubeconfig\n\n创建配置文件cat &gt; kube-proxy.yaml &lt;&lt; &quot;EOF&quot;\napiVersion: kubeproxy.config.k8s.io&#x2F;v1alpha1\nbindAddress: 10.10.0.78\nclientConnection:\n  kubeconfig: &#x2F;etc&#x2F;kubernetes&#x2F;kube-proxy.kubeconfig\nclusterCIDR: 172.168.0.0&#x2F;12\nhealthzBindAddress: 192.168.2.18:10256\nkind: KubeProxyConfiguration\nmetricsBindAddress: 192.168.2.18:10249\nmode: &quot;ipvs&quot;\nEOF\n\n创建启动文件cat &gt;  kube-proxy.service &lt;&lt; &quot;EOF&quot;\n[Unit]\nDescription&#x3D;Kubernetes Kube-Proxy Server\nDocumentation&#x3D;https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;kubernetes\nAfter&#x3D;network.target\n\n[Service]\nWorkingDirectory&#x3D;&#x2F;var&#x2F;lib&#x2F;kube-proxy\nExecStart&#x3D;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;kube-proxy \\\n  --config&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;kube-proxy.yaml \\\n  --alsologtostderr&#x3D;true \\\n  --logtostderr&#x3D;false \\\n  --log-dir&#x3D;&#x2F;var&#x2F;log&#x2F;kubernetes \\\n  --v&#x3D;2\nRestart&#x3D;on-failure\nRestartSec&#x3D;5\nLimitNOFILE&#x3D;65536\n\n[Install]\nWantedBy&#x3D;multi-user.target\nEOF\n\n同步文件cp kube-proxy*.pem &#x2F;etc&#x2F;kubernetes&#x2F;ssl&#x2F;\ncp kube-proxy.kubeconfig kube-proxy.yaml &#x2F;etc&#x2F;kubernetes&#x2F;\ncp kube-proxy.service &#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;\nfor i in msater2 master3 node1 node2 node3;do scp  kube-proxy.kubeconfig kube-proxy.yaml $i:&#x2F;etc&#x2F;kubernetes&#x2F;;done\nfor i in  msater2 master3 node1 node2 node3;do scp  kube-proxy.service $i:&#x2F;usr&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;;done\n\n修改IPvim &#x2F;etc&#x2F;kubernetes&#x2F;kube-proxy.yaml\n\n启动服务mkdir -p &#x2F;var&#x2F;lib&#x2F;kube-proxy\nsystemctl daemon-reload\nsystemctl enable --now kube-proxy\n\nsystemctl status kube-proxy\n\n（八）部署网络组件部署calico创建calico-etcd文件cat calico-etcd.yaml &gt;&gt;&gt; EOF\n---\n# Source: calico&#x2F;templates&#x2F;calico-etcd-secrets.yaml\n# The following contains k8s Secrets for use with a TLS enabled etcd cluster.\n# For information on populating Secrets, see http:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;user-guide&#x2F;secrets&#x2F;\napiVersion: v1\nkind: Secret\ntype: Opaque\nmetadata:\n  name: calico-etcd-secrets\n  namespace: kube-system\ndata:\n  # Populate the following with etcd TLS configuration if desired, but leave blank if\n  # not using TLS for etcd.\n  # The keys below should be uncommented and the values populated with the base64\n  # encoded contents of each file that would be associated with the TLS data.\n  # Example command for encoding a file contents: cat &lt;file&gt; | base64 -w 0\n  # etcd-key: null\n  # etcd-cert: null\n  # etcd-ca: null\n---\n# Source: calico&#x2F;templates&#x2F;calico-config.yaml\n# This ConfigMap is used to configure a self-hosted Calico installation.\nkind: ConfigMap\napiVersion: v1\nmetadata:\n  name: calico-config\n  namespace: kube-system\ndata:\n  # Configure this with the location of your etcd cluster.\n  etcd_endpoints: &quot;http:&#x2F;&#x2F;&lt;ETCD_IP&gt;:&lt;ETCD_PORT&gt;&quot;\n  # If you&#39;re using TLS enabled etcd uncomment the following.\n  # You must also populate the Secret below with these files.\n  etcd_ca: &quot;&quot;   # &quot;&#x2F;calico-secrets&#x2F;etcd-ca&quot;\n  etcd_cert: &quot;&quot; # &quot;&#x2F;calico-secrets&#x2F;etcd-cert&quot;\n  etcd_key: &quot;&quot;  # &quot;&#x2F;calico-secrets&#x2F;etcd-key&quot;\n  # Typha is disabled.\n  typha_service_name: &quot;none&quot;\n  # Configure the backend to use.\n  calico_backend: &quot;bird&quot;\n  # Configure the MTU to use for workload interfaces and tunnels.\n  # - If Wireguard is enabled, set to your network MTU - 60\n  # - Otherwise, if VXLAN or BPF mode is enabled, set to your network MTU - 50\n  # - Otherwise, if IPIP is enabled, set to your network MTU - 20\n  # - Otherwise, if not using any encapsulation, set to your network MTU.\n  veth_mtu: &quot;1440&quot;\n\n  # The CNI network configuration to install on each node. The special\n  # values in this config will be automatically populated.\n  cni_network_config: |-\n    &#123;\n      &quot;name&quot;: &quot;k8s-pod-network&quot;,\n      &quot;cniVersion&quot;: &quot;0.3.1&quot;,\n      &quot;plugins&quot;: [\n        &#123;\n          &quot;type&quot;: &quot;calico&quot;,\n          &quot;log_level&quot;: &quot;info&quot;,\n          &quot;etcd_endpoints&quot;: &quot;__ETCD_ENDPOINTS__&quot;,\n          &quot;etcd_key_file&quot;: &quot;__ETCD_KEY_FILE__&quot;,\n          &quot;etcd_cert_file&quot;: &quot;__ETCD_CERT_FILE__&quot;,\n          &quot;etcd_ca_cert_file&quot;: &quot;__ETCD_CA_CERT_FILE__&quot;,\n          &quot;mtu&quot;: __CNI_MTU__,\n          &quot;ipam&quot;: &#123;\n              &quot;type&quot;: &quot;calico-ipam&quot;\n          &#125;,\n          &quot;policy&quot;: &#123;\n              &quot;type&quot;: &quot;k8s&quot;\n          &#125;,\n          &quot;kubernetes&quot;: &#123;\n              &quot;kubeconfig&quot;: &quot;__KUBECONFIG_FILEPATH__&quot;\n          &#125;\n        &#125;,\n        &#123;\n          &quot;type&quot;: &quot;portmap&quot;,\n          &quot;snat&quot;: true,\n          &quot;capabilities&quot;: &#123;&quot;portMappings&quot;: true&#125;\n        &#125;,\n        &#123;\n          &quot;type&quot;: &quot;bandwidth&quot;,\n          &quot;capabilities&quot;: &#123;&quot;bandwidth&quot;: true&#125;\n        &#125;\n      ]\n    &#125;\n---\n# Source: calico&#x2F;templates&#x2F;calico-kube-controllers-rbac.yaml\n\n# Include a clusterrole for the kube-controllers component,\n# and bind it to the calico-kube-controllers serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io&#x2F;v1\nmetadata:\n  name: calico-kube-controllers\nrules:\n  # Pods are monitored for changing labels.\n  # The node controller monitors Kubernetes nodes.\n  # Namespace and serviceaccount labels are used for policy.\n  - apiGroups: [&quot;&quot;]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n      - serviceaccounts\n    verbs:\n      - watch\n      - list\n      - get\n  # Watch for changes to Kubernetes NetworkPolicies.\n  - apiGroups: [&quot;networking.k8s.io&quot;]\n    resources:\n      - networkpolicies\n    verbs:\n      - watch\n      - list\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io&#x2F;v1\nmetadata:\n  name: calico-kube-controllers\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-kube-controllers\nsubjects:\n- kind: ServiceAccount\n  name: calico-kube-controllers\n  namespace: kube-system\n---\n\n---\n# Source: calico&#x2F;templates&#x2F;calico-node-rbac.yaml\n# Include a clusterrole for the calico-node DaemonSet,\n# and bind it to the calico-node serviceaccount.\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io&#x2F;v1\nmetadata:\n  name: calico-node\nrules:\n  # The CNI plugin needs to get pods, nodes, and namespaces.\n  - apiGroups: [&quot;&quot;]\n    resources:\n      - pods\n      - nodes\n      - namespaces\n    verbs:\n      - get\n  - apiGroups: [&quot;&quot;]\n    resources:\n      - endpoints\n      - services\n    verbs:\n      # Used to discover service IPs for advertisement.\n      - watch\n      - list\n  # Pod CIDR auto-detection on kubeadm needs access to config maps.\n  - apiGroups: [&quot;&quot;]\n    resources:\n      - configmaps\n    verbs:\n      - get\n  - apiGroups: [&quot;&quot;]\n    resources:\n      - nodes&#x2F;status\n    verbs:\n      # Needed for clearing NodeNetworkUnavailable flag.\n      - patch\n\n---\napiVersion: rbac.authorization.k8s.io&#x2F;v1\nkind: ClusterRoleBinding\nmetadata:\n  name: calico-node\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: calico-node\nsubjects:\n- kind: ServiceAccount\n  name: calico-node\n  namespace: kube-system\n\n---\n# Source: calico&#x2F;templates&#x2F;calico-node.yaml\n# This manifest installs the calico-node container, as well\n# as the CNI plugins and network config on\n# each master and worker node in a Kubernetes cluster.\nkind: DaemonSet\napiVersion: apps&#x2F;v1\nmetadata:\n  name: calico-node\n  namespace: kube-system\n  labels:\n    k8s-app: calico-node\nspec:\n  selector:\n    matchLabels:\n      k8s-app: calico-node\n  updateStrategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 1\n  template:\n    metadata:\n      labels:\n        k8s-app: calico-node\n    spec:\n      nodeSelector:\n        kubernetes.io&#x2F;os: linux\n      hostNetwork: true\n      tolerations:\n        # Make sure calico-node gets scheduled on all nodes.\n        - effect: NoSchedule\n          operator: Exists\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - effect: NoExecute\n          operator: Exists\n      serviceAccountName: calico-node\n      # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a &quot;force\n      # deletion&quot;: https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;workloads&#x2F;pods&#x2F;pod&#x2F;#termination-of-pods.\n      terminationGracePeriodSeconds: 0\n      priorityClassName: system-node-critical\n      initContainers:\n        # This container installs the CNI binaries\n        # and CNI network config file on each node.\n        - name: install-cni\n          image: registry.cn-beijing.aliyuncs.com&#x2F;dotbalo&#x2F;cni:v3.15.3\n          command: [&quot;&#x2F;install-cni.sh&quot;]\n          env:\n            # Name of the CNI config file to create.\n            - name: CNI_CONF_NAME\n              value: &quot;10-calico.conflist&quot;\n            # The CNI network config to install on each node.\n            - name: CNI_NETWORK_CONFIG\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: cni_network_config\n            # The location of the etcd cluster.\n            - name: ETCD_ENDPOINTS\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: etcd_endpoints\n            # CNI MTU Config variable\n            - name: CNI_MTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # Prevents the container from sleeping forever.\n            - name: SLEEP\n              value: &quot;false&quot;\n          volumeMounts:\n            - mountPath: &#x2F;host&#x2F;opt&#x2F;cni&#x2F;bin\n              name: cni-bin-dir\n            - mountPath: &#x2F;host&#x2F;etc&#x2F;cni&#x2F;net.d\n              name: cni-net-dir\n            - mountPath: &#x2F;calico-secrets\n              name: etcd-certs\n          securityContext:\n            privileged: true\n        # Adds a Flex Volume Driver that creates a per-pod Unix Domain Socket to allow Dikastes\n        # to communicate with Felix over the Policy Sync API.\n        - name: flexvol-driver\n          image: registry.cn-beijing.aliyuncs.com&#x2F;dotbalo&#x2F;pod2daemon-flexvol:v3.15.3\n          volumeMounts:\n          - name: flexvol-driver-host\n            mountPath: &#x2F;host&#x2F;driver\n          securityContext:\n            privileged: true\n      containers:\n        # Runs calico-node container on each Kubernetes node. This\n        # container programs network policy and routes on each\n        # host.\n        - name: calico-node\n          image: registry.cn-beijing.aliyuncs.com&#x2F;dotbalo&#x2F;node:v3.15.3\n          env:\n            # The location of the etcd cluster.\n            - name: ETCD_ENDPOINTS\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: etcd_endpoints\n            # Location of the CA certificate for etcd.\n            - name: ETCD_CA_CERT_FILE\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: etcd_ca\n            # Location of the client key for etcd.\n            - name: ETCD_KEY_FILE\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: etcd_key\n            # Location of the client certificate for etcd.\n            - name: ETCD_CERT_FILE\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: etcd_cert\n            # Set noderef for node controller.\n            - name: CALICO_K8S_NODE_REF\n              valueFrom:\n                fieldRef:\n                  fieldPath: spec.nodeName\n            # Choose the backend to use.\n            - name: CALICO_NETWORKING_BACKEND\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: calico_backend\n            # Cluster type to identify the deployment type\n            - name: CLUSTER_TYPE\n              value: &quot;k8s,bgp&quot;\n            # Auto-detect the BGP IP address.\n            - name: IP\n              value: &quot;autodetect&quot;\n            # Enable IPIP\n            - name: CALICO_IPV4POOL_IPIP\n              value: &quot;Always&quot;\n            # Enable or Disable VXLAN on the default IP pool.\n            - name: CALICO_IPV4POOL_VXLAN\n              value: &quot;Never&quot;\n            # Set MTU for tunnel device used if ipip is enabled\n            - name: FELIX_IPINIPMTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # Set MTU for the VXLAN tunnel device.\n            - name: FELIX_VXLANMTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # Set MTU for the Wireguard tunnel device.\n            - name: FELIX_WIREGUARDMTU\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: veth_mtu\n            # The default IPv4 pool to create on startup if none exists. Pod IPs will be\n            # chosen from this range. Changing this value after installation will have\n            # no effect. This should fall within &#96;--cluster-cidr&#96;.\n            # - name: CALICO_IPV4POOL_CIDR\n            #   value: &quot;192.168.0.0&#x2F;16&quot;\n            # Disable file logging so &#96;kubectl logs&#96; works.\n            - name: CALICO_DISABLE_FILE_LOGGING\n              value: &quot;true&quot;\n            # Set Felix endpoint to host default action to ACCEPT.\n            - name: FELIX_DEFAULTENDPOINTTOHOSTACTION\n              value: &quot;ACCEPT&quot;\n            # Disable IPv6 on Kubernetes.\n            - name: FELIX_IPV6SUPPORT\n              value: &quot;false&quot;\n            # Set Felix logging to &quot;info&quot;\n            - name: FELIX_LOGSEVERITYSCREEN\n              value: &quot;info&quot;\n            - name: FELIX_HEALTHENABLED\n              value: &quot;true&quot;\n          securityContext:\n            privileged: true\n          resources:\n            requests:\n              cpu: 250m\n          livenessProbe:\n            exec:\n              command:\n              - &#x2F;bin&#x2F;calico-node\n              - -felix-live\n              - -bird-live\n            periodSeconds: 10\n            initialDelaySeconds: 10\n            failureThreshold: 6\n          readinessProbe:\n            exec:\n              command:\n              - &#x2F;bin&#x2F;calico-node\n              - -felix-ready\n              - -bird-ready\n            periodSeconds: 10\n          volumeMounts:\n            - mountPath: &#x2F;lib&#x2F;modules\n              name: lib-modules\n              readOnly: true\n            - mountPath: &#x2F;run&#x2F;xtables.lock\n              name: xtables-lock\n              readOnly: false\n            - mountPath: &#x2F;var&#x2F;run&#x2F;calico\n              name: var-run-calico\n              readOnly: false\n            - mountPath: &#x2F;var&#x2F;lib&#x2F;calico\n              name: var-lib-calico\n              readOnly: false\n            - mountPath: &#x2F;calico-secrets\n              name: etcd-certs\n            - name: policysync\n              mountPath: &#x2F;var&#x2F;run&#x2F;nodeagent\n      volumes:\n        # Used by calico-node.\n        - name: lib-modules\n          hostPath:\n            path: &#x2F;lib&#x2F;modules\n        - name: var-run-calico\n          hostPath:\n            path: &#x2F;var&#x2F;run&#x2F;calico\n        - name: var-lib-calico\n          hostPath:\n            path: &#x2F;var&#x2F;lib&#x2F;calico\n        - name: xtables-lock\n          hostPath:\n            path: &#x2F;run&#x2F;xtables.lock\n            type: FileOrCreate\n        # Used to install CNI.\n        - name: cni-bin-dir\n          hostPath:\n            path: &#x2F;opt&#x2F;cni&#x2F;bin\n        - name: cni-net-dir\n          hostPath:\n            path: &#x2F;etc&#x2F;cni&#x2F;net.d\n        # Mount in the etcd TLS secrets with mode 400.\n        # See https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;configuration&#x2F;secret&#x2F;\n        - name: etcd-certs\n          secret:\n            secretName: calico-etcd-secrets\n            defaultMode: 0400\n        # Used to create per-pod Unix Domain Sockets\n        - name: policysync\n          hostPath:\n            type: DirectoryOrCreate\n            path: &#x2F;var&#x2F;run&#x2F;nodeagent\n        # Used to install Flex Volume Driver\n        - name: flexvol-driver-host\n          hostPath:\n            type: DirectoryOrCreate\n            path: &#x2F;usr&#x2F;libexec&#x2F;kubernetes&#x2F;kubelet-plugins&#x2F;volume&#x2F;exec&#x2F;nodeagent~uds\n---\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-node\n  namespace: kube-system\n\n---\n# Source: calico&#x2F;templates&#x2F;calico-kube-controllers.yaml\n# See https:&#x2F;&#x2F;github.com&#x2F;projectcalico&#x2F;kube-controllers\napiVersion: apps&#x2F;v1\nkind: Deployment\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n  labels:\n    k8s-app: calico-kube-controllers\nspec:\n  # The controllers can only have a single active instance.\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: calico-kube-controllers\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      name: calico-kube-controllers\n      namespace: kube-system\n      labels:\n        k8s-app: calico-kube-controllers\n    spec:\n      nodeSelector:\n        kubernetes.io&#x2F;os: linux\n      tolerations:\n        # Mark the pod as a critical add-on for rescheduling.\n        - key: CriticalAddonsOnly\n          operator: Exists\n        - key: node-role.kubernetes.io&#x2F;master\n          effect: NoSchedule\n      serviceAccountName: calico-kube-controllers\n      priorityClassName: system-cluster-critical\n      # The controllers must run in the host network namespace so that\n      # it isn&#39;t governed by policy that would prevent it from working.\n      hostNetwork: true\n      containers:\n        - name: calico-kube-controllers\n          image: registry.cn-beijing.aliyuncs.com&#x2F;dotbalo&#x2F;kube-controllers:v3.15.3\n          env:\n            # The location of the etcd cluster.\n            - name: ETCD_ENDPOINTS\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: etcd_endpoints\n            # Location of the CA certificate for etcd.\n            - name: ETCD_CA_CERT_FILE\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: etcd_ca\n            # Location of the client key for etcd.\n            - name: ETCD_KEY_FILE\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: etcd_key\n            # Location of the client certificate for etcd.\n            - name: ETCD_CERT_FILE\n              valueFrom:\n                configMapKeyRef:\n                  name: calico-config\n                  key: etcd_cert\n            # Choose which controllers to run.\n            - name: ENABLED_CONTROLLERS\n              value: policy,namespace,serviceaccount,workloadendpoint,node\n          volumeMounts:\n            # Mount in the etcd TLS secrets.\n            - mountPath: &#x2F;calico-secrets\n              name: etcd-certs\n          readinessProbe:\n            exec:\n              command:\n              - &#x2F;usr&#x2F;bin&#x2F;check-status\n              - -r\n      volumes:\n        # Mount in the etcd TLS secrets with mode 400.\n        # See https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;configuration&#x2F;secret&#x2F;\n        - name: etcd-certs\n          secret:\n            secretName: calico-etcd-secrets\n            defaultMode: 0400\n\n---\n\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: calico-kube-controllers\n  namespace: kube-system\n\n---\n# Source: calico&#x2F;templates&#x2F;calico-typha.yaml\n\n---\n# Source: calico&#x2F;templates&#x2F;configure-canal.yaml\n\n---\n# Source: calico&#x2F;templates&#x2F;kdd-crds.yaml\nEOF\n\n修改calico-etcd文件sed -i &#39;s#etcd_endpoints: &quot;http:&#x2F;&#x2F;&lt;ETCD_IP&gt;:&lt;ETCD_PORT&gt;&quot;#etcd_endpoints: &quot;https:&#x2F;&#x2F;192.168.2.18:2379,https:&#x2F;&#x2F;192.168.2.19:2379,https:&#x2F;&#x2F;192.168.2.20:2379&quot;#g&#39; calico-etcd.yaml\n\nETCD_CA&#x3D;&#96;cat &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd-ca.pem | base64 | tr -d &#39;\\n&#39;&#96;\nETCD_CERT&#x3D;&#96;cat &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd.pem | base64 | tr -d &#39;\\n&#39;&#96;\nETCD_KEY&#x3D;&#96;cat &#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;etcd&#x2F;etcd-key.pem | base64 | tr -d &#39;\\n&#39;&#96;\n\nsed -i &quot;s@# etcd-key: null@etcd-key: $&#123;ETCD_KEY&#125;@g; s@# etcd-cert: null@etcd-cert: $&#123;ETCD_CERT&#125;@g; s@# etcd-ca: null@etcd-ca: $&#123;ETCD_CA&#125;@g&quot; calico-etcd.yaml\n\n\nsed -i &#39;s#etcd_ca: &quot;&quot;#etcd_ca: &quot;&#x2F;calico-secrets&#x2F;etcd-ca&quot;#g; s#etcd_cert: &quot;&quot;#etcd_cert: &quot;&#x2F;calico-secrets&#x2F;etcd-cert&quot;#g; s#etcd_key: &quot;&quot; #etcd_key: &quot;&#x2F;calico-secrets&#x2F;etcd-key&quot; #g&#39; calico-etcd.yaml\n\n# 更改此处为自己的pod网段\nPOD_SUBNET&#x3D;&quot;172.16.0.0&#x2F;12&quot;\n\nsed -i &#39;s@# - name: CALICO_IPV4POOL_CIDR@- name: CALICO_IPV4POOL_CIDR@g; s@#   value: &quot;192.168.0.0&#x2F;16&quot;@  value: &#39;&quot;$&#123;POD_SUBNET&#125;&quot;&#39;@g&#39; calico-etcd.yaml\n\nkubectl apply -f calico-etcd.yaml\n\n# 查看容器状态\nkubectl  get po -n kube-system\n\n查看状态kubectl get pods -A\nkubectl get nodes\n\n部署corednsgit clone https:&#x2F;&#x2F;github.com&#x2F;coredns&#x2F;deployment.git\ncd deployment&#x2F;kubernetes\n.&#x2F;deploy.sh -s -i 10.96.0.10 | kubectl apply -f -\n查看状态\n # kubectl get po -n kube-system -l k8s-app&#x3D;kube-dns\n\n（九）部署Metrics  Servercat metrics-server.yml &gt;&gt;&gt; EOF\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server\n  namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io&#x2F;v1\nkind: ClusterRole\nmetadata:\n  labels:\n    k8s-app: metrics-server\n    rbac.authorization.k8s.io&#x2F;aggregate-to-admin: &quot;true&quot;\n    rbac.authorization.k8s.io&#x2F;aggregate-to-edit: &quot;true&quot;\n    rbac.authorization.k8s.io&#x2F;aggregate-to-view: &quot;true&quot;\n  name: system:aggregated-metrics-reader\nrules:\n  - apiGroups:\n      - metrics.k8s.io\n    resources:\n      - pods\n      - nodes\n    verbs:\n      - get\n      - list\n      - watch\n---\napiVersion: rbac.authorization.k8s.io&#x2F;v1\nkind: ClusterRole\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: system:metrics-server\nrules:\n  - apiGroups:\n      - &quot;&quot;\n    resources:\n      - pods\n      - nodes\n      - nodes&#x2F;stats\n      - namespaces\n      - configmaps\n    verbs:\n      - get\n      - list\n      - watch\n---\napiVersion: rbac.authorization.k8s.io&#x2F;v1\nkind: RoleBinding\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server-auth-reader\n  namespace: kube-system\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: Role\n  name: extension-apiserver-authentication-reader\nsubjects:\n  - kind: ServiceAccount\n    name: metrics-server\n    namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io&#x2F;v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server:system:auth-delegator\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:auth-delegator\nsubjects:\n  - kind: ServiceAccount\n    name: metrics-server\n    namespace: kube-system\n---\napiVersion: rbac.authorization.k8s.io&#x2F;v1\nkind: ClusterRoleBinding\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: system:metrics-server\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:metrics-server\nsubjects:\n  - kind: ServiceAccount\n    name: metrics-server\n    namespace: kube-system\n---\napiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server\n  namespace: kube-system\nspec:\n  ports:\n    - name: https\n      port: 443\n      protocol: TCP\n      targetPort: https\n  selector:\n    k8s-app: metrics-server\n---\napiVersion: apps&#x2F;v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: metrics-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: metrics-server\n  strategy:\n    rollingUpdate:\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        k8s-app: metrics-server\n    spec:\n      containers:\n        - args:\n            - --cert-dir&#x3D;&#x2F;tmp\n            - --secure-port&#x3D;4443\n            - --metric-resolution&#x3D;30s\n            - --kubelet-insecure-tls\n            - --kubelet-preferred-address-types&#x3D;InternalIP,ExternalIP,Hostname\n            - --requestheader-client-ca-file&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;pki&#x2F;front-proxy-ca.pem # change to front-proxy-ca.crt for kubeadm\n            - --requestheader-username-headers&#x3D;X-Remote-User\n            - --requestheader-group-headers&#x3D;X-Remote-Group\n            - --requestheader-extra-headers-prefix&#x3D;X-Remote-Extra-\n          image: registry.cn-beijing.aliyuncs.com&#x2F;dotbalo&#x2F;metrics-server:v0.4.1\n          imagePullPolicy: IfNotPresent\n          livenessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: &#x2F;livez\n              port: https\n              scheme: HTTPS\n            periodSeconds: 10\n          name: metrics-server\n          ports:\n            - containerPort: 4443\n              name: https\n              protocol: TCP\n          readinessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: &#x2F;readyz\n              port: https\n              scheme: HTTPS\n            periodSeconds: 10\n          securityContext:\n            readOnlyRootFilesystem: true\n            runAsNonRoot: true\n            runAsUser: 1000\n          volumeMounts:\n            - mountPath: &#x2F;tmp\n              name: tmp-dir\n            - name: ca-ssl\n              mountPath: &#x2F;etc&#x2F;kubernetes&#x2F;pki\n      nodeSelector:\n        kubernetes.io&#x2F;os: linux\n      priorityClassName: system-cluster-critical\n      serviceAccountName: metrics-server\n      volumes:\n        - emptyDir: &#123;&#125;\n          name: tmp-dir\n        - name: ca-ssl\n          hostPath:\n            path: &#x2F;etc&#x2F;kubernetes&#x2F;pki\n---\napiVersion: apiregistration.k8s.io&#x2F;v1\nkind: APIService\nmetadata:\n  labels:\n    k8s-app: metrics-server\n  name: v1beta1.metrics.k8s.io\nspec:\n  group: metrics.k8s.io\n  groupPriorityMinimum: 100\n  insecureSkipTLSVerify: true\n  service:\n    name: metrics-server\n    namespace: kube-system\n  version: v1beta1\n  versionPriority: 100\nEOF\n\n创建metrics-server.yml\nkubectl  create -f metrics-server.yml\n\n查看节点状态\nkubectl  top nodes\n","slug":"高可用部署k8s集群（二进制）","date":"2023-04-14T14:28:03.000Z","categories_index":"","tags_index":"Kubernetes","author_index":"xiaomaomi"},{"id":"a978a5e93d8e6628e9f4ee713be55be8","title":"Redis","content":"什么是RedisRedis是一个基于内存支持key-value等多种数据结构的存储系统，支持网络 可用于缓存，事件发布或订阅，高速队列等场景，支持字符串（String），哈希（Hashes），列表（Lists），队列（Sets），有序集合（Zset）等数据类型 基于内存 可持久化\n为什么使用Redis\n高性能Redis能读的速度是110000次&#x2F;s，写的速度是81000次&#x2F;s\n数据结构Redis支持二进制的String，Lists，Hashes，Sets，Sorted sets数据类型操作\n原子性Redis的所有操作都是原子性的，同时Redis还支持对几个操作全并后的原子性执行\n丰富的特性Redis支持Publicsh&#x2F;subscribe，通知key过期特性\n持久化Redis支持RDB，AOF等持久化方式\n发布订阅Redis支持发布&#x2F;订阅模式\n主从复制Redis\n分片\n可移植性\n\nRedis数据结构\n\n\n结构类型\n结构存储的值\n结构的读写能力\n\n\n\nString字符串\n可以是字符串，整数或浮点数\n对整个字符串或字符串的一部分解析操作，对整数或浮点数进行自增或自减操作\n\n\nList列表\n一个链表，链表上的每个节点都包含一个字符串\n对链表的俩端进行push和pop操作，读取单个或多个元素，根据值查找或删除元素\n\n\nSet集合\n包含字符串的无序集合\n字符串的集合，包括基础的方法是否存在添加，获取，删除；还包括计算交集。并集，差集等\n\n\nHash散列\n包含键值对的无序散列表\n包含方法有添加，获取，删除单个元素\n\n\nZset有序集合\n包含字符串的有序集合\n字符串成员与浮点数分数之间的有序映射，元素的排列顺序由分数的大小决定，包含方法有添加，获取，删除单个元素以及根据分值范围或成员来获取元素\n\n\nString 字符串String 字符串是redis最基本的数据类型，一个key对应一个value，一个键值对最大存储512MB\n\n\n\n命令格式\n功能\n案列\n\n\n\nset key value\n将key-value缓存redis中\nset name maomi\n\n\nget key\n从redis中获取key对应value值\nget name\n\n\nincr key\n将key对应的value值+1\nincr age\n\n\ndecr key\n将key对应的value值-1\ndecr age\n\n\nsetex key seconds value\n将key-value缓存到redis中，seconds秒后失效\nset miao 10 man\n\n\nttl key\n查看key存活空间\nttl miao\n\n\ndel key\n从redis中删除key\ndel name\n\n\nsetnx ket value\n如果key已经存，不做任何操作，如果key不存在，则直接添加\nsetnx name xiaomaomi\n\n\nHash类型Hash类型是String类型的fileld和value的映射表，特别适合存储对象 ，每个哈希键值中可以存储多达40亿个字段值对\n\n\n\n命令格式\n功能\n案列\n\n\n\nhset key field value\n将field value 对缓存到redis中hash中，键值为key\nhash user name maomi\n\n\nhget key field\n从key对应hash列表中获取field字段\nhget user name\n\n\nhexists key field\n判断key对应的hash列表是否存在field字段\nhexists user age\n\n\nhdel key field\n删除key对应hash列表中field字段\nhdel user age\n\n\nhincrby key field incrment\n给key对应hash列表中field字段+increment\nhincrby user age 10\n\n\nhlen key\n查看key对应的hash列表中field的数量\nhlen user\n\n\nhkeys key\n获取key对应的hash列表中所有的field值\nhkeys user\n\n\nhvals key\n获取key对应的hash列表中所有的field对应的value值\nkvals user\n\n\nhgetall key\n获取key对应的hash列表中所有的field及其对应的values值\nhgetall user\n\n\nList类型List链表是按插入顺序排序的字符串列表。可以在列表的头部（左）或尾部（右）添加元素，列表可以包含超过40亿个元素\n\n\n\n命令格式\n功能\n案例\n\n\n\nrpush key value\n从右边往key集合中添加value值\nrpush xiaomaomi baimao\n\n\nlrange key start stop\n从左边开始列表key集合，从start位置开始 stop位置结束\nlrarnge xiaomaomi 0 -1\n\n\nlpush key value\n从左边往key集合中添加value值\nlpush xiaomaomi heimao\n\n\nlpop key\n弹出key集合中最左边的数据\nlpop xiaomaomi\n\n\nrpop key\n弹出key集合中最右边的数据\nrepop xiaomaomi\n\n\nllen key\n获取列表长度\nllen xiaomaomi\n\n\nSets类型Set集合是String类型的无序集合，set是通过HashTable实现的 对集合我们可以取并集，交集，差集\n\n\n\n命令格式\n功能\n案例\n\n\n\nsadd key members[…..]\n往key集合中添加member元素\nsadd xiaomaomi buou xianluo  lihua\n\n\nsmembers key\n遍历key集合中所有的元素\nsmembers xiaomaomi\n\n\nsrem key members[…..]\n删除key集合中members元素\nsrem xiaomaomi buou\n\n\nspop key count\n从key集合中随机弹出count个元素\nspop xiaomaomi 1\n\n\nZset类型Zset集合是String类型的有序集合，有序集合的成员是唯一的，但是分数（score）允许重复\n\n\n\n命令格式\n功能\n案例\n\n\n\nzadd key score member\n往key集合中添加member元素 分数为score\nzadd players 100 a\n\n\nzincrby key increment member\n将key集合中的member元素 分数 + increment\nzadd players 100 a\n\n\nzrange key start stop [withscores]\n将key集合中的元素按分数升序排练 [显示分数]\nzrange players 0 -1 withscores\n\n\nzrevrange key start stop [withscores]\n将key集合中的分数降序排列 [显示分数]\nzrevrange players 0 -1 withscores\n\n\nzrank key member\n返回member元素在key结合中的正序排名\nzrank players a\n\n\nzrevrank key member\n返回member元素在key结合中的降序排名\nzrevrank players a\n\n\nzcard key\n返回key集合元素个数\nzcard players\n\n\n","slug":"Redis","date":"2023-04-12T09:42:37.000Z","categories_index":"","tags_index":"Redis","author_index":"xiaomaomi"},{"id":"8ddfacea4372ca3a2e64aca468ca4eb9","title":"raid","content":"什么是RAID？RAID（Redundant Array of Independent Disks） 即独立磁盘冗余阵列，简称为【磁盘阵列】 ，用多个独立磁盘组成一个大的磁盘系统，从而实现比单块磁盘更好的存储性能和更高的可靠性RAID主要利用数据条带，镜像和数据校验技术来获取高性能，可靠性，容错能力和扩展性，根据运用或组合运用这三种技术的策略和架构，可以把RAID分为不同的等级，以满足不同数据应用的需求，如RAID0，RAID1，RAID5，RAID6，RAID10等\nRAID特点\n高性能\n可用性和可靠性\n大容量\n可管理性\n\n软件RAID和硬件RAID\n软件RAID的性能较低 因为其使用主机的资源。\n硬件RAID的性能较高，它采用PCI Express 卡物理地提供有专用的RAID控制器，不会使用主机的资源\n\n重要的RAID概念\n校验方式用在RAID重建中从校验所保存的信息中重新生成丢失的内容。RAID 5，RAID 6基于校验\n条带化是将切片数据随机存储到多个磁盘，它不会在单个磁盘中保存完整的数据。RAID 0基于条带化\n镜像会自动备份数据，RAID 1和RAID 10 基于镜像\n热备份 服务器上的一个备用驱动器，可以自动更换发生故障的驱动器\n块是RAID控制器，每次读写数据时的最小单位，最小 4kb。通过定义块大小，我们可以增加I&#x2F;O性能\n\nRAID基本原理\n镜像（Mirroring）将数据复制到多个磁盘，一方面可以提升可靠性，另一方面可并发从俩个或多个副本读取数据来提高性能。\n数据条带（Data Stripping）将数据分片保存在多个不同的磁盘，多个数据分片共同组成一个完整数据副本，这与镜像的多个副本是不同的，它通常用于性能考虑。数据条带具有更高的并发粒度，当访问数据时，可以同时对位于不同磁盘闪光数据进行读写操作，从而获得非常可观的I&#x2F;O性能提升\n数据校验（Data Pariy）利用冗余数据进行数据错误检测和修复，冗余数据通常采用海明码，异或操作等算法来计算获得。利用校验功能，可以很大程度上提高磁盘阵列的可靠性和容错能力。数据校验功能需要从多处读取数据并进行计算和对比 会影响性能\n\nRAID方案\nRAID 0（条带化）条带化有很好的性能 在RAID0中数据使用切片的方式被写入到磁盘，一半内容放在一个磁盘上 另一半内容放在另一个磁盘上，在这种情况上 任意一个磁盘驱动器发生故障，都会丢失数据，因为一个盘中只有一半的数据，不能用于重建RAID 0，但是RAID 0 对于读写性能很好\n\n高性能\nRAID 0 中容量零损失\n零容错\n写和读有很高性能\n\n\nRAID1（镜像化）镜像也有不错的性能，镜像可以对我们的数据做一份相同的副本，但是会丢失一半可用空间。如果发生磁盘故障，我们可用通过更换一个新的磁盘恢复RAID，RAID 1 一个磁盘故障 可以从另一个磁盘中获取数据 数据零丢失\n\n良好的性能\n总容量丢失一半可用空间\n完全容错\n重建会更快\n写性能变慢\n读性能变好\n能用于操作系统和小规模的数据库\n\n\nRAID 5（分布式奇偶校验）RAID 5 多用于企业级，奇偶校验用于重建数据，它从剩下的正常驱动器上信息来重建，如果一个驱动器故障了 我们可以从奇偶校验中恢复数据，奇偶校验存储在所有驱动器上，单个驱动器故障 RAID5继续运行 超过1个就会造成数据丢失\n\n性能高\n读取速度非常好\n写入速度处于平均水准 如果不是硬件RAID 写速度缓慢\n从所有驱动器的奇偶校验信息中重建数据\n完全容错\n1个磁盘空间用于奇偶校验\n可以被用在文件服务器，Web服务器，非常重要的备份中\n\n\nRAID6（双分布式奇偶校验磁盘）RAID 6 与RAID 5 相似但它有俩个分布式奇偶校验，大多数用在大数量的阵列中，最少需要4个驱动器，即使有俩个驱动器故障 可以更换新的驱动器来重建数据\n\n性能不佳\n读取性能好\n不使用硬件RAID控制器写的性能很差\n从俩个奇偶校验驱动器上重建\n完全容错\n2个磁盘空间用作奇偶校验\n可用于大型阵列\n用于备份和视频流中，用于大规模\n\n\nRAID10（镜像+条带）RAID 10也被称为镜像阵列带。像RAID 0 一样跨数据盘抽取 像RAID 1 一样 每个磁盘都有一个镜像磁盘 RAID 10提供100%的数据冗余 支持更大的卷尺寸，但价格相对较高。\n\n良好的性能\n总容量丢失一半的可用空间\n完全容错\n从副本数据中快速重建\n由于其高性能和高可用性，常被用于数据库的存储中\n\n\n\n","slug":"raid","date":"2023-04-08T00:40:45.000Z","categories_index":"","tags_index":"Linux","author_index":"xiaomaomi"}]